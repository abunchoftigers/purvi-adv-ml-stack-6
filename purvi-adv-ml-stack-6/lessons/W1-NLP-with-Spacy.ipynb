{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1d6325e93f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the filepath\n",
    "# fpath= \"Data/Dracula.txt\"\n",
    "# # Use with open syntax\n",
    "# with open(fpath) as f:\n",
    "#     txt = f.read()\n",
    "# # Report length of the text\n",
    "# print(f\"There are {len(txt)} characters in the full text.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps in the pipeline\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable ner\n",
    "nlp_no_ner = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "# Print active components\n",
    "nlp_no_ner.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    }
   ],
   "source": [
    "# define text for demonstration\n",
    "sample_text = \"While running in Central Park, \\nI noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a doc with the nlp pipeline\n",
    "doc = nlp(sample_text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "While running in Central Park, \n",
       "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokens in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While\n",
      "running\n",
      "in\n",
      "Central\n",
      "Park\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      "noticed\n",
      "a\n",
      "discarded\n",
      "McDonald\n",
      "'s\n",
      "container\n",
      ",\n",
      "surounded\n",
      "by\n",
      "buzzing\n",
      "flies\n",
      "was\n",
      "annoying\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 10 tokens separately\n",
    "for token in doc:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "running"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a token from the doc\n",
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is stop word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is punctuation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is whitespace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.text</th>\n",
       "      <th>.lemma_</th>\n",
       "      <th>.pos_</th>\n",
       "      <th>.is_stop</th>\n",
       "      <th>.is_punct</th>\n",
       "      <th>.is_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While</td>\n",
       "      <td>while</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central</td>\n",
       "      <td>Central</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Park</td>\n",
       "      <td>Park</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noticed</td>\n",
       "      <td>notice</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     .text  .lemma_  .pos_  .is_stop  .is_punct  .is_space\n",
       "0    While    while  SCONJ      True      False      False\n",
       "1  running      run   VERB     False      False      False\n",
       "2       in       in    ADP      True      False      False\n",
       "3  Central  Central  PROPN     False      False      False\n",
       "4     Park     Park  PROPN     False      False      False\n",
       "5        ,        ,  PUNCT     False       True      False\n",
       "6       \\n       \\n  SPACE     False      False       True\n",
       "7        I        I   PRON      True      False      False\n",
       "8  noticed   notice   VERB     False      False      False\n",
       "9        a        a    DET      True      False      False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create dictionary for desired attributes for each token\n",
    "token_data = []\n",
    "for token in doc:\n",
    "    token_dict = {\n",
    "        \".text\": token.text,\n",
    "        \".lemma_\": token.lemma_,\n",
    "        \".pos_\": token.pos_,\n",
    "        \".is_stop\": token.is_stop,\n",
    "        \".is_punct\": token.is_punct,\n",
    "        \".is_space\": token.is_space\n",
    "    }\n",
    "    token_data.append(token_dict)\n",
    "# Save dictionary as a dataframe\n",
    "spacy_df = pd.DataFrame(token_data) \n",
    "spacy_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', ',', '\\n', 'noticed', 'discarded', 'mcdonald', 'container', ',', 'surounded', 'buzzing', 'flies', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# For loop to remove stopwords\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword, skip it\n",
    "    if token.is_stop == True:\n",
    "        continue \n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtaining lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_lemmas = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "\n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # # keep the tokens'.text for the final list of tokens\n",
    "        # cleaned_tokens.append(token.text.lower())\n",
    "        # keep the tokens's .lemma_ for the final list of tokens\n",
    "        cleaned_lemmas.append(token.lemma_.lower())\n",
    "        \n",
    "print(cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare cleane text and the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words:\n",
      " ['running', 'central', 'park', ',', '\\n', 'noticed', 'discarded', 'mcdonald', 'container', ',', 'surounded', 'buzzing', 'flies', 'annoying', '.'] \n",
      "\n",
      "Lemmatized words:\n",
      " ['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Compare text and lemmas\n",
    "print(\"Tokenized words:\\n\", cleaned_tokens,\"\\n\")\n",
    "print(\"Lemmatized words:\\n\", cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc, remove_stopwords=True, remove_punct=True, use_lemmas=False):\n",
    "    \"\"\"Temporary Fucntion - for Education Purposes (we will make something better below)\n",
    "    \"\"\"\n",
    "    tokens = [ ]\n",
    "    for token in doc:\n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_stopwords == True) and (token.is_stop == True):\n",
    "            # Continue the loop with the next token\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_punct == True):\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_space == True):\n",
    "            continue\n",
    "    \n",
    "        ## Determine final form of output list of tokens/lemmas\n",
    "        if use_lemmas:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'running', 'in', 'central', 'park', ',', '\\n', 'i', 'noticed', 'a', 'discarded', 'mcdonald', \"'s\", 'container', ',', 'surounded', 'by', 'buzzing', 'flies', 'was', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to a doc.\n",
    "doc = nlp(sample_text)\n",
    "# Tokenizing, keeping stopwords and punctuatin\n",
    "dirty_tokens = preprocess_doc(doc, remove_stopwords=False,remove_punct=False)\n",
    "print(dirty_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing, removing stopwords and punctuation\n",
    "cleaned_tokens = preprocess_doc(doc, remove_stopwords=True,remove_punct=True)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing, removing stopwords and punctuation\n",
    "cleaned_lemmas = preprocess_doc(doc, remove_stopwords=True,remove_punct=True, use_lemmas=True)\n",
    "print(cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Framework (Not runnable)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lists_of_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mtext1\u001b[49m, text2, text3]\n\u001b[0;32m      3\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe(list_of_texts):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text1' is not defined"
     ]
    }
   ],
   "source": [
    "# # Example Framework (Not runnable)\n",
    "# lists_of_texts = [text1, text2, text3]\n",
    "# processed_texts = []\n",
    "# for doc in nlp.pipe(list_of_texts):\n",
    "#     doc_tokens = []\n",
    "#     for token in doc:\n",
    "#         # ... the same logic from our preprocess docs function.\n",
    "#         doc_tokens.append(token.text.lower())\n",
    "        \n",
    "#     # Append the list of tokens for current doc to processed_texts\n",
    "#     processed_texts.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    processed_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:28, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Default args will produce tokens\n",
    "tokens = batch_preprocess_texts([sample_text])\n",
    "tokens = tokens[0]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting use_lemmas = True will produce lemmas\n",
    "lemmas = batch_preprocess_texts([sample_text], use_lemmas=True)\n",
    "lemmas = lemmas[0]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:33, 33.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting use_lemmas = True will produce lemmas\n",
    "lemmas = batch_preprocess_texts([sample_text], use_lemmas=True)\n",
    "lemmas = lemmas[0]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. However, I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "doc = nlp(sample_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting a list of sentences from the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting sentences from doc\n",
    "sentences = list(doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central Park LOC\n",
      "McDonald ORG\n"
     ]
    }
   ],
   "source": [
    "# Print any named entities in the doc and its label\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dojo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
